{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bias and Variance\n",
    "### Definitions:\n",
    "Bias: bias is a feature of a statistical technique, it describes the gap between expected results and estimated values.\n",
    "Variance: variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value.\n",
    "### Why bias and variance\n",
    "Someone may ask there are so many metrics to evaluate the performence of model, taking linear regression example, we usually use rmse to estimat the performance. Should we analyze the rmse to improve the performance? Of course you can, but for the purposes of deciding how to make progress on specific problems, bias and variance are sufficient. \n",
    "\n",
    "Understanding them will help us make decisions when the performance of selected model is not well. By adding more training data or using different tactics to improve performance, bias and variance can tell me something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Underfitting and Overfitting\n",
    "### overfitting\n",
    "Suppose the algorithm performs as follows:\n",
    "\n",
    "•Training_error=1%\n",
    "\n",
    "•Testing_error=11%\n",
    "\n",
    "At here ,we can see the bias of the algorithm is 1%, and the variance is 10%(11%-1%),and this is a typical style of overfitting,which has a good accuray in training but not in testing.\n",
    "### underfitting\n",
    "• Training_error = 15%\n",
    "\n",
    "• Testing_error = 16%\n",
    "\n",
    "For this algorithm,we can see the bias is 15% but the variance is 1%, we say this phenomena is high bias,or underfitting,which is not fit well even in training set.\n",
    "### overfitting and underfitting\n",
    "consider this:\n",
    "\n",
    "• Training_error = 15%\n",
    "\n",
    "• Testing_error = 30%\n",
    "\n",
    "we can see that both train error and testing error are high, high bias and high variance,this is both overfitting and underfitting, which menas the algorithm may not cathch the pattern from the given model.\n",
    "### how to improve ML algorithms\n",
    "underfitting: 1)adding more features\n",
    "\n",
    "overfitting:1)get more training data; 2)regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Gradient Descent\n",
    "Here is the picture depict in ML course,we have learned in last course, but what is the problems of gradient descent?\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/gradient_descent.PNG\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 tuning learning rates\n",
    "In practice, we should tune the learning rates carefully, for it can affact the whole outcome drammatically. If the learning rate is small, it spends too much time to get the optimization,otherwise, if that is big enouhg, we may miss the minimum value. See the picture below:\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/loss_function.JPG\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Someone may ask if the data's demension is more than 2D, how can we make learning rate with weight? Surely, it is hard to depict complete figures in practice, but we can make iteration with loss function figures,just like below.\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/loss_iter.JPG\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "At the begining,we are far away from destination, so we hope we can move fast,that means having large learning rate;however,after several epoches, we get close to destination, so moving slowly may be a good choice.One thing we should keep in mind, that learning rates can not be one size fits all, we need to change parameters depending on specific situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stochastic Gradient Descent\n",
    "The main difference between gradient descent with stochastic gradient descent is lie on how to calculate loss functions,which the former uses the whole examples to calculate once but the latter using one example each time, this can make algorithm faster. THe following figure shows how it works.\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/stochastic.JPG\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn SDG to fit training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3666006525842923"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "clf.fit(X, y)\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "    num_complete_batch=math.floor(m/mini_batch_size)\n",
    "    for i in range(num_complete_batch):\n",
    "        mini_batch_X=shuffled_X[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "        mini_batch_Y=shuffled_Y[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "        mini_batch=(mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X=shuffled_X[:,num_complete_batch*mini_batch_size:]\n",
    "        mini_batch_Y=shuffled_Y[:,num_complete_batch*mini_batch_size:]\n",
    "        mini_batch=(mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling\n",
    "The main idea of feature scaling is to make different features have the same range. The common methods are StandardScaler and MinMaxScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 cross validation\n",
    "### 4.1 what is cross validation\n",
    "cross validation is a method which split training data into two parts, one part for training set to train model, and the others for testing set to evaluate the performance of model.\n",
    "### 4.2 why cross validation\n",
    "1. learn some useful messages from given dataset\n",
    "\n",
    "2. avoid overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
