{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is machine learning\n",
    "\n",
    "There are so many definitions about machine learning that we can not remember every word of them. Lee said in slide in a  simple way that machine learning is to find a best function for a given dataset. Actually, I think the most concrete description is experience,practice,task and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Knowledge\n",
    "### 2.1 Central limit theorem\n",
    "\"In probability theory, the central limit theorem (CLT) states that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed\", these sentences come from Wiki. I think the key of the central limit theorem lies on big samples, and on this basic,we can find data distribution and use mathematic methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normal distribution\n",
    "Normal distribution is a common distribution in data analysis. We describe normal distribution using parameters like mean and standard deviations. In fact, all normal distributions have the common properies which called mean,median and standard deviations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Maximum likelihood estimation\n",
    "Maximum likelihood estimation deal with the problem: we get the model(may be we know the data follows a normal distribution), but we do not know the parameters(like mean and standard) except a finite dataset. What we do is to find the maximum parameters with specfic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 derivative or others \n",
    "I think in this cource, we need not to know so many details about derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 annotation: \n",
    "y is a vector,which means a set of yi,and the same as W and X\n",
    "\n",
    "$x^n_i$:i represents the ith training number and the n represents the nth features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 loss function and gradient descent\n",
    "$y=WX+b=w_1*x_1+...+w_n*x_n+b=\\sum_{i=1}^{n}{w_i*x_i}+b$ \n",
    "After parameters W and b are initialized, you can do the 'loss function' and \"gradient descent\" propagation steps for learning the parameters.\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $ypred = w^T X + b = (ypred^{(0)}, ypred^{(1)}, ..., ypred^{(m-1)}, ypred^{(m)})$\n",
    "- You calculate the cost function: $J = \\frac{1}{m}\\sum_{i=0}^{m}(ypred^{(i)}-y^{(i)})^2$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(ypred-y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_pred^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class linear_regression:\n",
    "    def __init__(self,n,max_iter,alpha):\n",
    "        self.max_iter=max_iter\n",
    "        self.learning_rate=alpha\n",
    "        self.W=np.zeros([n,1])\n",
    "        self.b=np.zeros([n,1])\n",
    "        \n",
    "    def Hypothesis(self,X):\n",
    "        Z=np.dot(X,self.W.T)+self.b\n",
    "        return Z\n",
    "    \n",
    "    def compute_cost(self,X,y):\n",
    "        y_pred=self.Hypothesis(X)\n",
    "        loss=np.sum((y_pred-y)**2)/m\n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent(self,X,y):\n",
    "        m,n=X.shape\n",
    "        dW=np.zeros([n,1])\n",
    "        db=np.zeros([n,1])\n",
    "        dw=1/m*np.dot(X.T,self.Hypothesis(X)-y)\n",
    "        db=1/m*np.sum(self.Hypothesis(X)-y)\n",
    "        return dw,db\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        for i in range(self.max_iter):\n",
    "            new_w=self.W-self.gradient_descent(X,y)[0]*self.learning_rate\n",
    "            new_b=self.b-self.gradient_descent(X,y)[1]*self.learning_rate\n",
    "            self.w=new_w\n",
    "            self.b=new_b\n",
    "            \n",
    "    def rmse(self,X,y):\n",
    "        rmse=np.sqrt(self.compute_cost(X,y))\n",
    "        return rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "X_train = diabetes_X[:-20]\n",
    "X_test = diabetes_X[-20:]\n",
    "y_train = diabetes.target[:-20].reshape(-1,1)\n",
    "y_test = diabetes.target[-20:].reshape(-1,1)\n",
    "m,n=X_train.shape\n",
    "reg = linear_regression(n,1000,0.001)\n",
    "reg.fit(X_train,y_train)\n",
    "rmse=reg.rmse(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization\n",
    "### 4.1 Why regularized is useful?\n",
    "In practice, the performence of the model may unfavorable,that is to say overfit or underfit. To make the model more reliable and improve the performence, we may introduce regularization. The reason why regularize is useful, I understand it through the way like this: if the model overfit, which means there are over features to fit the choose model,by adding the L-norm multipliy lambda, we choose the lambda big enough, when calculate cost function and gradient descent, W will change to small,so the features will lose its values;otherwise, if the model underfit, which means lacking features, so lambda is small, nearly 0, w will increse.\n",
    "### 4.2 L0,L1 and L2\n",
    "L0:corresponds to the total number of nonzero elements in a vector.\n",
    "L1:corresponds to the sum of the magnitudes of the vectors in a space.\n",
    "L2:the most popular norm, also known as the Euclidean norm. It is the shortest distance to go from one point to another.\n",
    "### 4.3 liner regression regularized\n",
    "$J = \\frac{1}{m}\\sum_{i=0}^{m}(ypred^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2*m}\\sum_{j=1}^{m}w^2_j$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(ypred-y)^T+\\frac{\\lambda}{m}w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,lambd,n,max_iter,alpha):\n",
    "    self.lambd=lambd\n",
    "    self.max_iter=max_iter\n",
    "    self.learning_rate=alpha\n",
    "    self.W=np.zeros([n,1])\n",
    "    self.b=np.zeros([n,1])\n",
    "    \n",
    "def compute_cost_with_regularized(self,X,y):\n",
    "    x,n=X.shape\n",
    "    y_pred=self.Hypothesis(X)\n",
    "    loss=np.sum((y_pred-y)**2)/m+self.lambd/(2*m)*np.sum(self.W**2)\n",
    "    return loss\n",
    "\n",
    "def gradient_descent_with_regularized(self,X,y):\n",
    "    m,n=X.shape\n",
    "    dW=np.zeros([n,1])\n",
    "    db=np.zeros([n,1])\n",
    "    dw=1/m*np.dot(X.T,self.Hypothesis(X)-y)+self.lambd/m*w\n",
    "    db=1/m*np.sum(self.Hypothesis(X)-y)\n",
    "    return dw,db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
