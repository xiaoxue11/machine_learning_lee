{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "1. 推导LR损失函数(1)\n",
    "2. 学习LR梯度下降(2)\n",
    "3. 利用代码描述梯度下降(选做)(3)\n",
    "4. Softmax原理(4)\n",
    "5. softmax损失函数(5)\n",
    "6. softmax梯度下降(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. logistic regression\n",
    "logistic regression 是一种分类模型，由条件概率分布P(Y|X)表示。典型的二分类模型中，随机变量X取值为实数，随机变量Y取值为0或者1。实际应用中，我们通过监督学习的方法评估模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LR梯度下降\n",
    "\n",
    "### 2.1 sigmoid 求导\n",
    "$$ sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "基础知识：\n",
    "$\\frac{d(e^x)}{dx}=e^x$\n",
    "\n",
    "$$\\frac{d(sigmoid(x))}{dx}=\\frac{(1+e^{-x})*e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}*(1-\\frac{1}{1+e^{-x}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 损失函数求偏微分\n",
    "$$ z=\\theta_0+\\theta_1*x_1+...+\\theta_i*x_i+\\theta_n*x_n=X*\\theta$$\n",
    "\n",
    "$$h(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}[\\sum_{i=1}^{m}{y^{(i)}*\\ln h(z^{(i)})+(1-y^{(i)})*\\ln (1-h(z^{(i)}))}] $$\n",
    "\n",
    "$$\\frac{dz(\\theta)}{d\\theta}=x$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta}=-\\frac{1}{m}[\\sum_{i=1}^{m}{y^{(i)}*\\frac{1}{h(z^{(i)})}*\\frac{d(h(z^{(i)}))}{d\\theta}+(1-y^{(i)})*(\\frac{-1}{1-h(z^{(i)})})*\\frac{d(h(z^{(i)}))}{d\\theta}}]$$\n",
    "\n",
    "$$ =-\\frac{1}{m}[\\sum_{i=1}^{m}{y^{(i)}*(1-h(z^{(i)}))*x{(i)}+(1-y^{(i)}))*h(z^{(i)})*x^{(i)}}]$$\n",
    "\n",
    "$$=-\\frac{1}{m}[\\sum_{i=1}^{m}{(y^{(i)}-h(z^{(i)}))*x^{(i)}}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Logistic_Regression:\n",
    "    def __init__(self,max_iter,learning_rate):\n",
    "        self.max_iter=max_iter\n",
    "        self.lr=learning_rate\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        y=y.reshape(-1,1)\n",
    "        X=np.c_[np.ones(m),X]\n",
    "        self.W=np.ones([n+1,1])\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred=self.sigmoid(np.dot(X,self.W))\n",
    "            grad=np.dot(X.T,y_pred-y)\n",
    "            new_W=self.W-self.lr*grad\n",
    "            self.W=new_W\n",
    "            \n",
    "    def predict(self,X):\n",
    "        m=X.shape[0]\n",
    "        X=np.c_[np.ones(m),X]\n",
    "        y_pred=np.zeros([m,1])\n",
    "        result=self.sigmoid(np.dot(X,self.W))\n",
    "        for i in range(m):\n",
    "            if result[i,0]>0.5:\n",
    "                y_pred[i]=1\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        m=X_test.shape[0]\n",
    "        y_pred=self.predict(X_test)\n",
    "        y_test=y_test.reshape(-1,1)\n",
    "        count=0\n",
    "        for i in range(m):\n",
    "            if y_pred[i]==y_test[i]:\n",
    "                count+=1\n",
    "        return count/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Softmax\n",
    "### 4.1 Softmax原理\n",
    "假设有三个类别c1,c2,c3，权重和偏差分别为w1,b1,w2,b2,w3,b3\n",
    "</table>\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/softmax.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "    如上图：Softmax做的事情就是将结果进行exponential，将exponential 的结果相加，再分别用 exponential 的结果除以相加的结果。原本z1,z2,z3z1,z2,z3可以是任何值，但做完Softmax之后输出会被限制住，都介于0到1之间，并且和是1。最终输出的结果最大值即为当前预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 为什么要取对数？\n",
    "<统计学方法>一书中有详细介绍最大熵模型，详细解释为什么取对数有效，可参见书籍中最大熵模型的推导\n",
    "</table>\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/1.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "</table>\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/2.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.softmax损失函数\n",
    "</table>\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/soft.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. softmax梯度下降\n",
    "</table>\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/det.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "    \n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
